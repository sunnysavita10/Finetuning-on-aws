{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884b052-85a9-4215-a664-8ec28eef6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adfeedb-e7ce-437b-a7b8-d2d5601e0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e9f1b9-81a2-422f-89b4-46923b5da14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd29d05-1614-4441-9c5c-84e19892612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.huggingface import get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27463297-f8b3-4042-8ab3-8ea0083ac86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ee99ae-254c-45d1-9329-a0977aa5d947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::385046010545:role/SageMakerLLMRole'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eab4d86-cd3a-4174-a183-b3484f101305",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_id\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
    "    \"epochs\": 2,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"lr\": 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268b8e0c-928a-420d-921f-a7427b4ad502",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./scripts\",\n",
    "    role=role,\n",
    "    transformers_version=\"4.36\",\n",
    "    pytorch_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=\"s3://llm-model-artifacts-santosh/models/\",\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee276d1-6bd5-4f81-95aa-cd3e6b18d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2025-11-16-09-59-43-092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 09:59:43 Starting - Starting the training job\n",
      "2025-11-16 09:59:43 Pending - Training job waiting for capacity...\n",
      "2025-11-16 10:00:05 Pending - Preparing the instances for training...\n",
      "2025-11-16 10:00:33 Downloading - Downloading input data...\n",
      "2025-11-16 10:00:54 Downloading - Downloading the training image........................\n",
      "2025-11-16 10:04:51 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:09,890 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:09,909 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:09,920 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:09,927 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:11,977 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:12,012 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:12,042 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:12,053 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"lr\": 2e-05,\n",
      "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2025-11-16-09-59-43-092\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://llm-model-artifacts-santosh/huggingface-pytorch-training-2025-11-16-09-59-43-092/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"lr\":2e-05,\"model_id\":\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://llm-model-artifacts-santosh/huggingface-pytorch-training-2025-11-16-09-59-43-092/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"lr\":2e-05,\"model_id\":\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2025-11-16-09-59-43-092\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://llm-model-artifacts-santosh/huggingface-pytorch-training-2025-11-16-09-59-43-092/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--lr\",\"2e-05\",\"--model_id\",\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LR=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --epochs 2 --lr 2e-05 --model_id TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T --per_device_train_batch_size 2\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:12,054 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:12,054 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===== Loading Dataset =====\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5 examples [00:00, 1206.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/5 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 5/5 [00:00<00:00, 1561.89 examples/s]\u001b[0m\n",
      "\u001b[34m===== Loading Tokenizer =====\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/5 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 5/5 [00:00<00:00, 689.78 examples/s]\u001b[0m\n",
      "\u001b[34m===== Loading Base Model in 4bit =====\u001b[0m\n",
      "\u001b[34m===== Applying QLoRA =====\u001b[0m\n",
      "\u001b[34m===== Setting Training Params =====\u001b[0m\n",
      "\u001b[34m===== Starting Training =====\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m50%|█████     | 1/2 [00:01<00:01,  1.35s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.00it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2.1038, 'train_samples_per_second': 4.753, 'train_steps_per_second': 0.951, 'train_loss': 3.7419145107269287, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.00it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34m===== Saving Model =====\u001b[0m\n",
      "\u001b[34m===== Training Completed Successfully =====\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:58,232 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:58,232 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-11-16 10:05:58,232 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-11-16 10:06:20 Uploading - Uploading generated training model\n",
      "2025-11-16 10:06:20 Completed - Training job completed\n",
      "Training seconds: 346\n",
      "Billable seconds: 346\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\n",
    "    \"train\": \"s3://llm-finetune-dataset-santosh/datasets/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb6d1e-1560-4d7c-b762-f4f4f605a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.latest_training_job.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb8a30d-2694-48a6-90d0-03ac56169cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://llm-model-artifacts-santosh/models/huggingface-pytorch-training-2025-11-16-09-59-43-092/output/model.tar.gz'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fd413-eb7c-40bf-bc00-22dd95bcbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to check all the accessible services inside your AWS Sagemaker\n",
    "\n",
    "# from sagemaker import image_uris\n",
    "\n",
    "# image_uris.retrieve(\n",
    "#     framework=\"huggingface\",\n",
    "#     region=\"ap-south-1\",   # change your region\n",
    "#     version=\"4.37.0\",\n",
    "#     image_scope=\"inference\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d8b01-60cc-44f3-881d-1c8d99b1247a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # instance_type=\"ml.g5.xlarge\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac725ff-5315-42a3-8891-594b40101911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HuggingFaceModel(\n",
    "#     model_data=\"s3://bucket/model.tar.gz\",\n",
    "#     role=role,\n",
    "#     entry_point=\"inference.py\",\n",
    "#     source_dir=\"inference\",\n",
    "#     transformers_version=\"4.36\",\n",
    "#     pytorch_version=\"2.1\",\n",
    "#     py_version=\"py310\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9469eb0-6c05-47e4-baba-8ce6eb5fef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## the code for the deployment\n",
    "# import sagemaker\n",
    "# from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# role = sagemaker.get_execution_role()\n",
    "\n",
    "# model = HuggingFaceModel(\n",
    "#     model_data=\"s3://llm-model-artifacts-sunny/models/huggingface-pytorch-training-2025-11-15-11-42-14-536/output/model.tar.gz\",\n",
    "#     role=role,\n",
    "#     transformers_version=\"4.37.0\",\n",
    "#     pytorch_version=\"2.1.0\",\n",
    "#     py_version=\"py310\",\n",
    "# )\n",
    "\n",
    "# predictor = model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "#     endpoint_name=\"live-finetune-endpoint\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec311cf2-b0f6-4f7b-8007-d5d030d905d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://llm-model-artifacts-santosh/models/huggingface-pytorch-training-2025-11-16-09-59-43-092/output/model.tar.gz), script artifact (./scripts), and dependencies ([]) into single tar.gz file located at s3://sagemaker-ap-south-1-385046010545/huggingface-pytorch-inference-2025-11-16-12-41-16-475/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-inference-2025-11-16-12-41-18-163\n",
      "INFO:sagemaker:Creating endpoint-config with name santosh2-finetune-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name santosh2-finetune-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    model_data=\"s3://llm-model-artifacts-santosh/models/huggingface-pytorch-training-2025-11-16-09-59-43-092/output/model.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"./inference\",\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"2.1.0\",\n",
    "    py_version=\"py310\"\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    endpoint_name=\"santosh2-finetune-endpoint\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a626cbe9-49a7-4e7b-820f-19907b8532a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'Explain AWS S3 Bucket?\\nAWS S3 is a cloud storage service that allows users to store, manage, and distribute large amounts of data. S3 buckets are logical units of storage that are managed by AWS and can be shared by multiple users.\\nS3 buckets are designed to be scalable, reliable, and fast. They can be used to store static or dynamic content, such as web pages, images, videos, and documents.\\nAWS S3 buckets can be used to store data in the cloud without exposing it to the public internet. They can also be used to perform data access operations, such as listing, uploading, and downloading files.\\n1. AWS S3 Bucket is a cloud-based storage service that allows users to store, manage, and distribute large amounts of data.\\n2. AWS S3 Buckets are logical units of storage that are managed by AWS and can be shared by multiple users.\\n3. S3'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THIS IS JUST TO VALIDATE WHETHER MODEL WORKING OR NOT IN THE NOTEBOOK ITSELF\n",
    "predictor.predict({\"inputs\": \"Explain AWS S3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6870b-b6f5-47b7-9e7c-8f0badf2ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after the deployment URL will look like this\n",
    "https://runtime.sagemaker.<region>.amazonaws.com/endpoints/live-finetune-endpoint/invocations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
